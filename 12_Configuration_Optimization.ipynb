{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Agents SDK - 설정 관리와 최적화\n",
    "\n",
    "이 튜토리얼은 OpenAI Agents SDK에서 환경별 설정 관리와 성능/비용 최적화 방법을 다룬다. 개발/스테이징/프로덕션 환경에 맞는 에이전트 설정, 토큰 최적화, 응답 캐싱, 메모리 최적화 등의 핵심 패턴을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 패키지 설치\n",
    "!pip install openai-agents python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. 설정 관리 (Configuration Management)\n",
    "\n",
    "이 챕터에서는 환경별 에이전트 설정을 관리하는 방법을 다룬다. 개발, 스테이징, 프로덕션 환경에 따라 다른 모델, 온도, 토큰 제한 등을 적용하는 패턴을 학습한다.\n",
    "\n",
    "### 학습 내용\n",
    "\n",
    "- 다양한 배포 환경을 위한 AgentConfig 데이터클래스 구축\n",
    "- 환경별 에이전트 생성을 위한 팩토리 패턴\n",
    "- 환경별 모델 설정, 타임아웃, 비용 제한 관리\n",
    "- 프로덕션 환경을 위한 설정 관리 패턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, ModelSettings, Runner\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AgentConfig 데이터클래스\n",
    "\n",
    "에이전트 설정을 구조화하기 위한 데이터클래스를 정의한다. 이 클래스는 모델, 온도, 토큰 제한, 타임아웃, 트레이싱, 비용 제한 등의 설정을 포함한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentConfig 데이터클래스가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"\n",
    "    에이전트 배포 환경을 위한 설정 클래스이다.\n",
    "    \"\"\"\n",
    "    model: str                      # 사용할 모델\n",
    "    temperature: float              # 응답 다양성\n",
    "    max_tokens: int                 # 최대 출력 토큰\n",
    "    timeout: float                  # 요청 타임아웃 (초)\n",
    "    enable_tracing: bool            # 트레이싱 활성화 여부\n",
    "    cost_limit_per_request: float   # 요청당 비용 제한 (USD)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"AgentConfig(model={self.model}, temp={self.temperature}, \"\n",
    "            f\"max_tokens={self.max_tokens}, timeout={self.timeout}s)\"\n",
    "        )\n",
    "\n",
    "print(\"AgentConfig 데이터클래스가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경별 설정 팩토리\n",
    "\n",
    "개발(development), 스테이징(staging), 프로덕션(production) 환경에 맞는 설정을 제공하는 팩토리 클래스를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConfiguredAgentFactory 클래스가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "class ConfiguredAgentFactory:\n",
    "    \"\"\"\n",
    "    환경별 에이전트 생성을 위한 팩토리 클래스이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_config(environment: str = \"production\") -> AgentConfig:\n",
    "        \"\"\"\n",
    "        특정 환경에 대한 설정을 반환한다.\n",
    "        \n",
    "        Args:\n",
    "            environment: 환경 이름 (development, staging, production)\n",
    "        \n",
    "        Returns:\n",
    "            해당 환경의 AgentConfig\n",
    "        \"\"\"\n",
    "        \n",
    "        configs = {\n",
    "            \"development\": AgentConfig(\n",
    "                model=\"gpt-4o-mini\",      # 개발용으로 빠르고 저렴한 모델\n",
    "                temperature=0.7,           # 더 다양한 응답으로 테스트\n",
    "                max_tokens=1000,\n",
    "                timeout=30.0,\n",
    "                enable_tracing=True,       # 개발 시 상세 트레이싱\n",
    "                cost_limit_per_request=0.01\n",
    "            ),\n",
    "            \"staging\": AgentConfig(\n",
    "                model=\"gpt-4o\",\n",
    "                temperature=0.5,\n",
    "                max_tokens=1500,\n",
    "                timeout=45.0,\n",
    "                enable_tracing=True,\n",
    "                cost_limit_per_request=0.05\n",
    "            ),\n",
    "            \"production\": AgentConfig(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0.3,           # 프로덕션에서는 더 일관된 응답\n",
    "                max_tokens=2000,\n",
    "                timeout=60.0,\n",
    "                enable_tracing=False,      # 프로덕션에서는 오버헤드 감소\n",
    "                cost_limit_per_request=0.10\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        return configs.get(environment, configs[\"production\"])\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_agent(\n",
    "        name: str, \n",
    "        instructions: str, \n",
    "        environment: str = None,\n",
    "        tools: list = None\n",
    "    ) -> Agent:\n",
    "        \"\"\"\n",
    "        지정된 환경에 맞게 설정된 에이전트를 생성한다.\n",
    "        \n",
    "        Args:\n",
    "            name: 에이전트 이름\n",
    "            instructions: 에이전트 지시문\n",
    "            environment: 환경 (None이면 ENVIRONMENT 환경변수 사용)\n",
    "            tools: 에이전트 도구 목록\n",
    "        \n",
    "        Returns:\n",
    "            설정된 Agent 인스턴스\n",
    "        \"\"\"\n",
    "        \n",
    "        env = environment or os.getenv(\"ENVIRONMENT\", \"production\")\n",
    "        config = ConfiguredAgentFactory.get_config(env)\n",
    "        \n",
    "        return Agent(\n",
    "            name=name,\n",
    "            instructions=instructions,\n",
    "            model=config.model,\n",
    "            model_settings=ModelSettings(\n",
    "                temperature=config.temperature,\n",
    "                max_tokens=config.max_tokens\n",
    "            ),\n",
    "            tools=tools or []\n",
    "        )\n",
    "\n",
    "print(\"ConfiguredAgentFactory 클래스가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 환경별 설정 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 환경별 설정 비교 ===\n",
      "\n",
      "환경              모델              온도       토큰       타임아웃       트레이싱      \n",
      "----------------------------------------------------------------------\n",
      "development     gpt-4o-mini     0.7      1000     30.0       True      \n",
      "staging         gpt-4o          0.5      1500     45.0       True      \n",
      "production      gpt-4o-mini     0.3      2000     60.0       False     \n"
     ]
    }
   ],
   "source": [
    "print(\"=== 환경별 설정 비교 ===\")\n",
    "print()\n",
    "\n",
    "environments = [\"development\", \"staging\", \"production\"]\n",
    "\n",
    "print(f\"{'환경':<15} {'모델':<15} {'온도':<8} {'토큰':<8} {'타임아웃':<10} {'트레이싱':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for env in environments:\n",
    "    config = ConfiguredAgentFactory.get_config(env)\n",
    "    print(\n",
    "        f\"{env:<15} {config.model:<15} {config.temperature:<8} \"\n",
    "        f\"{config.max_tokens:<8} {config.timeout:<10} {str(config.enable_tracing):<10}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 팩토리를 사용한 에이전트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 환경별 에이전트 생성 ===\n",
      "\n",
      "[development]\n",
      "  에이전트: DevelopmentAgent\n",
      "  모델: gpt-4o-mini\n",
      "  설정: AgentConfig(model=gpt-4o-mini, temp=0.7, max_tokens=1000, timeout=30.0s)\n",
      "\n",
      "[staging]\n",
      "  에이전트: StagingAgent\n",
      "  모델: gpt-4o\n",
      "  설정: AgentConfig(model=gpt-4o, temp=0.5, max_tokens=1500, timeout=45.0s)\n",
      "\n",
      "[production]\n",
      "  에이전트: ProductionAgent\n",
      "  모델: gpt-4o-mini\n",
      "  설정: AgentConfig(model=gpt-4o-mini, temp=0.3, max_tokens=2000, timeout=60.0s)\n"
     ]
    }
   ],
   "source": [
    "# 다양한 환경에서 에이전트 생성\n",
    "print(\"=== 환경별 에이전트 생성 ===\")\n",
    "\n",
    "for env in [\"development\", \"staging\", \"production\"]:\n",
    "    agent = ConfiguredAgentFactory.create_agent(\n",
    "        name=f\"{env.capitalize()}Agent\",\n",
    "        instructions=\"당신은 전문적인 고객 서비스 에이전트입니다.\",\n",
    "        environment=env\n",
    "    )\n",
    "    config = ConfiguredAgentFactory.get_config(env)\n",
    "    print(f\"\\n[{env}]\")\n",
    "    print(f\"  에이전트: {agent.name}\")\n",
    "    print(f\"  모델: {config.model}\")\n",
    "    print(f\"  설정: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 프로덕션 에이전트 테스트 ===\n",
      "\n",
      "[질문] 계정 정보는 어떻게 업데이트하나요?\n",
      "[응답] 계정 정보를 업데이트하려면 다음 단계를 따라주세요:\n",
      "\n",
      "1. 웹사이트에 로그인합니다.\n",
      "2. '내 계정' 또는 '프로필' 섹션으로 이동합니다.\n",
      "3. 수정할 정보를 선택하고 변경합니다.\n",
      "4. 변경 사항을 저장합니다.\n",
      "\n",
      "필요한 경우 고객 지원에 문의하시면 추가 도움을 드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "# 프로덕션 에이전트 테스트\n",
    "print(\"=== 프로덕션 에이전트 테스트 ===\")\n",
    "\n",
    "prod_agent = ConfiguredAgentFactory.create_agent(\n",
    "    name=\"ProductionAgent\",\n",
    "    instructions=\"당신은 전문적인 고객 서비스 에이전트입니다. 간결하고 정확하게 답변하세요.\",\n",
    "    environment=\"production\"\n",
    ")\n",
    "\n",
    "result = await Runner.run(prod_agent, \"계정 정보는 어떻게 업데이트하나요?\")\n",
    "print(f\"\\n[질문] 계정 정보는 어떻게 업데이트하나요?\")\n",
    "print(f\"[응답] {result.final_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 토큰 최적화 (Token Optimization)\n",
    "\n",
    "이 챕터에서는 비용과 성능을 최적화하는 기법을 다룬다. 사용 사례별 에이전트 설정, 응답 캐싱, 배치 처리 등의 패턴을 학습한다.\n",
    "\n",
    "### 학습 내용\n",
    "\n",
    "- 사용 사례별 최적화된 모델 설정으로 에이전트 생성\n",
    "- API 호출과 비용을 줄이기 위한 응답 캐싱 구현\n",
    "- 효율성 향상을 위한 배치 처리\n",
    "- 성능 모니터링 및 캐시 적중률 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "환경 설정 완료. 토큰 최적화 데모를 시작한다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner, ModelSettings\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# 환경 변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# API 키 확인\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY를 찾을 수 없다. .env 파일을 확인하라.\")\n",
    "\n",
    "if not api_key.startswith(\"sk-\"):\n",
    "    raise ValueError(\"올바르지 않은 OpenAI API 키 형식이다.\")\n",
    "\n",
    "print(\"환경 설정 완료. 토큰 최적화 데모를 시작한다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OptimizedAgentManager 클래스\n",
    "\n",
    "사용 사례별 최적화와 응답 캐싱을 제공하는 에이전트 관리자 클래스를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedAgentManager 클래스가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "class OptimizedAgentManager:\n",
    "    \"\"\"\n",
    "    최적화된 에이전트 성능을 위한 관리자 클래스이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.response_cache = {}  # 응답 캐시\n",
    "        self.cache_hits = 0       # 캐시 적중 횟수\n",
    "        self.cache_misses = 0     # 캐시 미스 횟수\n",
    "    \n",
    "    def create_optimized_agent(self, use_case: str) -> Agent:\n",
    "        \"\"\"\n",
    "        특정 사용 사례에 최적화된 에이전트를 생성한다.\n",
    "        \n",
    "        Args:\n",
    "            use_case: 사용 사례 (quick_responses, detailed_analysis, creative_tasks)\n",
    "        \n",
    "        Returns:\n",
    "            최적화된 Agent 인스턴스\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizations = {\n",
    "            \"quick_responses\": {\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"settings\": ModelSettings(\n",
    "                    temperature=0.1,    # 일관된 짧은 응답\n",
    "                    max_tokens=150,     # 토큰 제한\n",
    "                    top_p=0.5\n",
    "                ),\n",
    "                \"instructions\": \"간결하고 직접적인 응답을 제공하세요. 짧고 정확하게 답변하세요.\"\n",
    "            },\n",
    "            \"detailed_analysis\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"settings\": ModelSettings(\n",
    "                    temperature=0.3,    # 분석적이고 일관된 응답\n",
    "                    max_tokens=2000,\n",
    "                    top_p=0.8\n",
    "                ),\n",
    "                \"instructions\": \"언어를 효율적으로 사용하면서 포괄적인 분석을 제공하세요.\"\n",
    "            },\n",
    "            \"creative_tasks\": {\n",
    "                \"model\": \"gpt-4o\",\n",
    "                \"settings\": ModelSettings(\n",
    "                    temperature=0.7,    # 창의적 다양성\n",
    "                    max_tokens=1000,\n",
    "                    top_p=0.9\n",
    "                ),\n",
    "                \"instructions\": \"창의적이지만 집중된 응답을 제공하세요. 불필요한 부연을 피하세요.\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = optimizations.get(use_case, optimizations[\"quick_responses\"])\n",
    "        \n",
    "        return Agent(\n",
    "            name=f\"Optimized_{use_case}\",\n",
    "            model=config[\"model\"],\n",
    "            model_settings=config[\"settings\"],\n",
    "            instructions=config[\"instructions\"]\n",
    "        )\n",
    "    \n",
    "    async def execute_with_caching(\n",
    "        self, \n",
    "        agent: Agent, \n",
    "        user_input: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        반복 쿼리를 위한 응답 캐싱과 함께 실행한다.\n",
    "        \n",
    "        Args:\n",
    "            agent: 실행할 에이전트\n",
    "            user_input: 사용자 입력\n",
    "        \n",
    "        Returns:\n",
    "            응답과 메타데이터를 담은 딕셔너리\n",
    "        \"\"\"\n",
    "        \n",
    "        # 간단한 캐시 키 (프로덕션에서는 더 정교한 해싱 사용)\n",
    "        cache_key = f\"{agent.name}:{hash(user_input)}\"\n",
    "        \n",
    "        if cache_key in self.response_cache:\n",
    "            self.cache_hits += 1\n",
    "            return {\n",
    "                \"response\": self.response_cache[cache_key],\n",
    "                \"cached\": True,\n",
    "                \"execution_time\": 0.001  # 캐시 응답 시간\n",
    "            }\n",
    "        \n",
    "        # 실행 및 결과 캐시\n",
    "        self.cache_misses += 1\n",
    "        start_time = time.time()\n",
    "        result = await Runner.run(agent, user_input)\n",
    "        execution_time = time.time() - start_time\n",
    "        \n",
    "        # 응답 캐시\n",
    "        self.response_cache[cache_key] = result.final_output\n",
    "        \n",
    "        return {\n",
    "            \"response\": result.final_output,\n",
    "            \"cached\": False,\n",
    "            \"execution_time\": execution_time,\n",
    "            \"token_usage\": result.raw_responses[0].usage.total_tokens if result.raw_responses else 0\n",
    "        }\n",
    "    \n",
    "    async def batch_execute(\n",
    "        self, \n",
    "        agent: Agent, \n",
    "        queries: List[str]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        배치로 여러 쿼리를 효율적으로 실행한다.\n",
    "        \n",
    "        Args:\n",
    "            agent: 실행할 에이전트\n",
    "            queries: 쿼리 목록\n",
    "        \n",
    "        Returns:\n",
    "            결과와 성능 메트릭을 담은 딕셔너리\n",
    "        \"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 모든 쿼리를 동시에 실행\n",
    "        tasks = [self.execute_with_caching(agent, query) for query in queries]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # 성능 메트릭 계산\n",
    "        cached_count = sum(1 for r in results if r[\"cached\"])\n",
    "        total_tokens = sum(r.get(\"token_usage\", 0) for r in results)\n",
    "        \n",
    "        return {\n",
    "            \"results\": results,\n",
    "            \"performance\": {\n",
    "                \"total_time\": total_time,\n",
    "                \"queries_processed\": len(queries),\n",
    "                \"cache_hit_rate\": cached_count / len(queries) * 100,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"avg_time_per_query\": total_time / len(queries)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"캐시 통계를 반환한다.\"\"\"\n",
    "        total = self.cache_hits + self.cache_misses\n",
    "        return {\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_misses\": self.cache_misses,\n",
    "            \"total_requests\": total,\n",
    "            \"hit_rate\": (self.cache_hits / total * 100) if total > 0 else 0,\n",
    "            \"cache_size\": len(self.response_cache)\n",
    "        }\n",
    "\n",
    "print(\"OptimizedAgentManager 클래스가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용 사례별 에이전트 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 사용 사례별 에이전트 설정 ===\n",
      "\n",
      "사용 사례                모델              온도       최대 토큰       \n",
      "------------------------------------------------------------\n",
      "quick_responses      gpt-4o-mini     0.1      150         \n",
      "detailed_analysis    gpt-4o          0.3      2000        \n",
      "creative_tasks       gpt-4o          0.7      1000        \n"
     ]
    }
   ],
   "source": [
    "# 사용 사례별 설정 비교\n",
    "print(\"=== 사용 사례별 에이전트 설정 ===\")\n",
    "print()\n",
    "\n",
    "manager = OptimizedAgentManager()\n",
    "\n",
    "use_cases = [\"quick_responses\", \"detailed_analysis\", \"creative_tasks\"]\n",
    "\n",
    "print(f\"{'사용 사례':<20} {'모델':<15} {'온도':<8} {'최대 토큰':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for use_case in use_cases:\n",
    "    agent = manager.create_optimized_agent(use_case)\n",
    "    # ModelSettings에서 설정 추출\n",
    "    settings = agent.model_settings\n",
    "    print(\n",
    "        f\"{use_case:<20} {agent.model:<15} \"\n",
    "        f\"{settings.temperature:<8} {settings.max_tokens:<12}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 캐싱과 배치 처리 데모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성능 최적화 시연\n",
      "==================================================\n",
      "\n",
      "--- 빠른 응답 최적화 (캐싱 포함) ---\n",
      "처리된 쿼리: 5개\n",
      "총 시간: 1.769초\n",
      "캐시 적중률: 0.0%\n",
      "쿼리당 평균 시간: 0.354초\n",
      "총 토큰: 234\n",
      "\n",
      "[개별 결과]\n",
      "1. [실행] 2+2는? -> 4입니다....\n",
      "2. [실행] 프랑스의 수도는? -> 파리입니다....\n",
      "3. [실행] AI 정의 -> AI(인공지능)는 인간의 지능을 모방하여 학습, 문제 해결, 의사결정 등을 수행하는 컴퓨터...\n",
      "4. [실행] 2+2는? -> 4입니다....\n",
      "5. [실행] 프랑스의 수도는? -> 파리입니다....\n"
     ]
    }
   ],
   "source": [
    "async def demonstrate_performance_optimization():\n",
    "    \"\"\"\n",
    "    성능 최적화 기법을 시연하는 함수이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"성능 최적화 시연\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    manager = OptimizedAgentManager()\n",
    "    \n",
    "    # 빠른 응답용 에이전트 생성\n",
    "    quick_agent = manager.create_optimized_agent(\"quick_responses\")\n",
    "    \n",
    "    # 테스트 쿼리 (중복 포함)\n",
    "    quick_queries = [\n",
    "        \"2+2는?\",\n",
    "        \"프랑스의 수도는?\",\n",
    "        \"AI 정의\",\n",
    "        \"2+2는?\",           # 중복 - 캐시 테스트\n",
    "        \"프랑스의 수도는?\"   # 중복 - 캐시 테스트\n",
    "    ]\n",
    "    \n",
    "    # 캐싱과 함께 빠른 응답 테스트\n",
    "    print(\"\\n--- 빠른 응답 최적화 (캐싱 포함) ---\")\n",
    "    quick_results = await manager.batch_execute(quick_agent, quick_queries)\n",
    "    \n",
    "    print(f\"처리된 쿼리: {quick_results['performance']['queries_processed']}개\")\n",
    "    print(f\"총 시간: {quick_results['performance']['total_time']:.3f}초\")\n",
    "    print(f\"캐시 적중률: {quick_results['performance']['cache_hit_rate']:.1f}%\")\n",
    "    print(f\"쿼리당 평균 시간: {quick_results['performance']['avg_time_per_query']:.3f}초\")\n",
    "    print(f\"총 토큰: {quick_results['performance']['total_tokens']}\")\n",
    "    \n",
    "    # 개별 결과 표시\n",
    "    print(\"\\n[개별 결과]\")\n",
    "    for i, (query, result) in enumerate(zip(quick_queries, quick_results['results'])):\n",
    "        status = \"[캐시]\" if result['cached'] else \"[실행]\"\n",
    "        print(f\"{i+1}. {status} {query} -> {result['response'][:50]}...\")\n",
    "    \n",
    "    return manager\n",
    "\n",
    "# 데모 실행\n",
    "manager = await demonstrate_performance_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 캐시 통계 ===\n",
      "  cache_hits: 0\n",
      "  cache_misses: 5\n",
      "  total_requests: 5\n",
      "  hit_rate: 0.0%\n",
      "  cache_size: 3\n"
     ]
    }
   ],
   "source": [
    "# 캐시 통계 확인\n",
    "print(\"\\n=== 캐시 통계 ===\")\n",
    "stats = manager.get_cache_stats()\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. 메모리 최적화 (Memory Optimization)\n",
    "\n",
    "이 챕터에서는 대화 메모리 최적화 기법을 다룬다. 긴 대화에서 토큰을 효율적으로 관리하고 중요한 컨텍스트를 보존하는 방법을 학습한다.\n",
    "\n",
    "### 학습 내용\n",
    "\n",
    "- 대화 기록 길이 관리를 위한 ContextOptimizer 구축\n",
    "- 모델 컨텍스트 제한 내에서의 토큰 기반 트리밍\n",
    "- 중복 정보 제거하면서 중요 컨텍스트 보존\n",
    "- 긴 세션 대화를 위한 성능 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ContextOptimizer 클래스\n",
    "\n",
    "대화 컨텍스트를 최적화하는 클래스를 정의한다. 메시지 수와 토큰 수 기반의 트리밍 기능을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ContextOptimizer 클래스가 정의되었다.\n"
     ]
    }
   ],
   "source": [
    "class ContextOptimizer:\n",
    "    \"\"\"\n",
    "    대화 컨텍스트를 최적화하여 성능을 향상시키는 클래스이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_context_length: int = 10):\n",
    "        \"\"\"\n",
    "        ContextOptimizer를 초기화한다.\n",
    "        \n",
    "        Args:\n",
    "            max_context_length: 유지할 최대 메시지 수\n",
    "        \"\"\"\n",
    "        self.max_context_length = max_context_length\n",
    "    \n",
    "    def optimize_conversation_history(self, history: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        성능 유지를 위해 대화 기록을 최적화한다.\n",
    "        시스템 메시지는 보존하고 최근 메시지를 유지한다.\n",
    "        \n",
    "        Args:\n",
    "            history: 대화 기록 리스트\n",
    "        \n",
    "        Returns:\n",
    "            최적화된 대화 기록\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(history) <= self.max_context_length:\n",
    "            return history\n",
    "        \n",
    "        # 시스템 메시지와 다른 메시지 분리\n",
    "        system_messages = [msg for msg in history if msg.get(\"role\") == \"system\"]\n",
    "        other_messages = [msg for msg in history if msg.get(\"role\") != \"system\"]\n",
    "        \n",
    "        # 최근 메시지 유지\n",
    "        keep_count = self.max_context_length - len(system_messages)\n",
    "        recent_messages = other_messages[-keep_count:]\n",
    "        \n",
    "        return system_messages + recent_messages\n",
    "    \n",
    "    def estimate_token_count(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        토큰 수를 대략적으로 추정한다.\n",
    "        \n",
    "        Args:\n",
    "            text: 추정할 텍스트\n",
    "        \n",
    "        Returns:\n",
    "            추정 토큰 수\n",
    "        \"\"\"\n",
    "        # 간단한 근사: 약 4자당 1토큰 (영어 기준)\n",
    "        # 한글은 약 2자당 1토큰으로 더 정확함\n",
    "        return len(text) // 3\n",
    "    \n",
    "    def trim_by_token_limit(\n",
    "        self, \n",
    "        history: List[Dict], \n",
    "        max_tokens: int = 4000\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        추정 토큰 수에 따라 대화 기록을 트리밍한다.\n",
    "        최근 메시지를 우선적으로 유지한다.\n",
    "        \n",
    "        Args:\n",
    "            history: 대화 기록 리스트\n",
    "            max_tokens: 최대 토큰 수\n",
    "        \n",
    "        Returns:\n",
    "            트리밍된 대화 기록\n",
    "        \"\"\"\n",
    "        \n",
    "        total_tokens = 0\n",
    "        trimmed_history = []\n",
    "        \n",
    "        # 역순으로 처리하여 최근 메시지 유지\n",
    "        for message in reversed(history):\n",
    "            content = str(message.get(\"content\", \"\"))\n",
    "            message_tokens = self.estimate_token_count(content)\n",
    "            \n",
    "            if total_tokens + message_tokens <= max_tokens:\n",
    "                trimmed_history.insert(0, message)\n",
    "                total_tokens += message_tokens\n",
    "            else:\n",
    "                # 시스템 메시지는 항상 포함\n",
    "                if message.get(\"role\") == \"system\":\n",
    "                    trimmed_history.insert(0, message)\n",
    "                break\n",
    "        \n",
    "        return trimmed_history\n",
    "    \n",
    "    def get_optimization_stats(\n",
    "        self, \n",
    "        original: List[Dict], \n",
    "        optimized: List[Dict]\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        최적화 통계를 반환한다.\n",
    "        \n",
    "        Args:\n",
    "            original: 원본 대화 기록\n",
    "            optimized: 최적화된 대화 기록\n",
    "        \n",
    "        Returns:\n",
    "            최적화 통계 딕셔너리\n",
    "        \"\"\"\n",
    "        original_tokens = sum(\n",
    "            self.estimate_token_count(str(m.get(\"content\", \"\"))) \n",
    "            for m in original\n",
    "        )\n",
    "        optimized_tokens = sum(\n",
    "            self.estimate_token_count(str(m.get(\"content\", \"\"))) \n",
    "            for m in optimized\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"original_messages\": len(original),\n",
    "            \"optimized_messages\": len(optimized),\n",
    "            \"messages_removed\": len(original) - len(optimized),\n",
    "            \"original_tokens\": original_tokens,\n",
    "            \"optimized_tokens\": optimized_tokens,\n",
    "            \"tokens_saved\": original_tokens - optimized_tokens,\n",
    "            \"reduction_rate\": (1 - optimized_tokens / original_tokens) * 100 if original_tokens > 0 else 0\n",
    "        }\n",
    "\n",
    "print(\"ContextOptimizer 클래스가 정의되었다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컨텍스트 최적화 데모"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 컨텍스트 최적화 데모 ===\n",
      "\n",
      "원본 대화 길이: 11 메시지\n",
      "메시지 수 기반 최적화 후: 5 메시지\n",
      "토큰 기반 트리밍 후 (100 토큰): 11 메시지\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_context_optimization():\n",
    "    \"\"\"\n",
    "    컨텍스트 최적화를 시연하는 함수이다.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== 컨텍스트 최적화 데모 ===\")\n",
    "    \n",
    "    optimizer = ContextOptimizer(max_context_length=5)\n",
    "    \n",
    "    # 긴 대화 기록 시뮬레이션\n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": \"당신은 도움이 되는 어시스턴트입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"안녕하세요!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"안녕하세요! 무엇을 도와드릴까요?\"},\n",
    "        {\"role\": \"user\", \"content\": \"농담 하나 해주세요.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"왜 프로그래머는 자연을 싫어할까요? 버그가 너무 많아서요!\"},\n",
    "        {\"role\": \"user\", \"content\": \"하나 더요.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"과학자들은 왜 원자를 믿지 않을까요? 모든 것을 만들어내거든요!\"},\n",
    "        {\"role\": \"user\", \"content\": \"감사합니다!\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"천만에요! 더 도움이 필요하시면 말씀해주세요.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬에 대해 알려주세요.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"파이썬은 읽기 쉽고 다재다능한 프로그래밍 언어입니다. 웹 개발, 데이터 과학, AI 등에 널리 사용됩니다.\"},\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n원본 대화 길이: {len(history)} 메시지\")\n",
    "    \n",
    "    # 메시지 수 기반 최적화\n",
    "    optimized_by_count = optimizer.optimize_conversation_history(history)\n",
    "    print(f\"메시지 수 기반 최적화 후: {len(optimized_by_count)} 메시지\")\n",
    "    \n",
    "    # 토큰 기반 트리밍\n",
    "    trimmed_by_tokens = optimizer.trim_by_token_limit(history, max_tokens=100)\n",
    "    print(f\"토큰 기반 트리밍 후 (100 토큰): {len(trimmed_by_tokens)} 메시지\")\n",
    "    \n",
    "    return optimizer, history, optimized_by_count\n",
    "\n",
    "optimizer, history, optimized = demonstrate_context_optimization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최적화 통계 ===\n",
      "  original_messages: 11\n",
      "  optimized_messages: 5\n",
      "  messages_removed: 6\n",
      "  original_tokens: 73\n",
      "  optimized_tokens: 39\n",
      "  tokens_saved: 34\n",
      "  reduction_rate: 46.6%\n"
     ]
    }
   ],
   "source": [
    "# 최적화 통계\n",
    "print(\"\\n=== 최적화 통계 ===\")\n",
    "stats = optimizer.get_optimization_stats(history, optimized)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최적화된 대화 내용 ===\n",
      "  [system] 당신은 도움이 되는 어시스턴트입니다.\n",
      "  [user] 감사합니다!\n",
      "  [assistant] 천만에요! 더 도움이 필요하시면 말씀해주세요.\n",
      "  [user] 파이썬에 대해 알려주세요.\n",
      "  [assistant] 파이썬은 읽기 쉽고 다재다능한 프로그래밍 언어입니다. 웹 개발, 데이터 과학, AI 등에 ...\n"
     ]
    }
   ],
   "source": [
    "# 최적화된 대화 내용 확인\n",
    "print(\"\\n=== 최적화된 대화 내용 ===\")\n",
    "for msg in optimized:\n",
    "    role = msg['role']\n",
    "    content = msg['content'][:50] + \"...\" if len(msg['content']) > 50 else msg['content']\n",
    "    print(f\"  [{role}] {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적화 모범 사례\n",
    "\n",
    "1. 설정 관리\n",
    "   - 환경별(개발/스테이징/프로덕션) 설정 분리\n",
    "   - 데이터클래스로 설정 구조화\n",
    "   - 팩토리 패턴으로 에이전트 생성 표준화\n",
    "\n",
    "2. 토큰 최적화\n",
    "   - 사용 사례별 최적화된 모델 설정\n",
    "   - 응답 캐싱으로 중복 요청 방지\n",
    "   - 배치 처리로 처리량 향상\n",
    "\n",
    "3. 메모리 최적화\n",
    "   - 대화 기록 길이 제한\n",
    "   - 토큰 기반 트리밍\n",
    "   - 시스템 메시지 항상 보존\n",
    "\n",
    "4. 비용 관리\n",
    "   - 요청당 비용 제한 설정\n",
    "   - 토큰 사용량 모니터링\n",
    "   - 개발 시 저렴한 모델 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 마무리\n",
    "\n",
    "이 튜토리얼에서는 OpenAI Agents SDK의 설정 관리와 최적화 기법을 다루었다. 다음과 같은 내용을 학습하였다:\n",
    "\n",
    "1. **설정 관리 (Configuration Management)**\n",
    "   - AgentConfig 데이터클래스로 설정 구조화\n",
    "   - ConfiguredAgentFactory로 환경별 에이전트 생성\n",
    "   - 개발/스테이징/프로덕션 환경별 최적 설정\n",
    "\n",
    "2. **토큰 최적화 (Token Optimization)**\n",
    "   - OptimizedAgentManager로 사용 사례별 에이전트 관리\n",
    "   - 응답 캐싱으로 중복 API 호출 방지\n",
    "   - 배치 처리로 동시 쿼리 효율적 처리\n",
    "\n",
    "3. **메모리 최적화 (Memory Optimization)**\n",
    "   - ContextOptimizer로 대화 기록 관리\n",
    "   - 메시지 수 및 토큰 기반 트리밍\n",
    "   - 시스템 메시지 보존하며 최근 컨텍스트 유지\n",
    "\n",
    "이러한 패턴을 적용하면 비용을 절감하고, 성능을 향상시키며, 다양한 환경에서 일관된 에이전트 동작을 보장할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph",
   "language": "python",
   "name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
